# --------------------
# Main Teacher Prompts
# --------------------

welcome_prompt = """
Hi there! ðŸ‘‹ Iâ€™m your private teacher for todayâ€”ready to help you learn, practice, and improve.
My main areas of expertise are Math, History, Science, and SAT questions, but youâ€™re welcome to ask me about other topics too.

What topic would you like to focus on today?
"""

# --------------------
# Question RAG Prompts
# --------------------


science_questions_prompt = """
Your science query should be in this format:
Category: <category>
Topic: <topic>
Skill: <skill>
Question_About: <free_text>

Example 1:
Category: Designing experiments
Topic: science-and-engineering-practices
Skill: Identify the experimental question

Example 2:
Category: Basic economic principles
Topic: economics
Skill: Trade and specialization

Example 3:
Category: Fossils
Topic: earth-science
Skill: Compare ages of fossils in a rock sequence

free_text could include any relevant information or context needed to answer the question.

Pay Attention! Your final output should be in a string in the format above.
"""

math_questions_prompt = """
Your math query should be in this format:
Topic: <topic>
Question_About: <free_text>

Where these are the available topics (with example in parentheses)
- algebra__linear_1d (e.g. Solve 24 = 1601*c - 1605*c for c.)
- algebra__polynomial_roots (e.g. Solve -3*w**3 + 1374*w**2 - 5433*w - 6810 = 0 for w.)
- arithmetic__add_or_sub (e.g. Total of 0.06 and -1977321735.)
- arithmetic__mul (e.g. Multiply 0 and -169144.)
- calculus__differentiate (e.g. What is the first derivative of 388896*d**3 - 222232?)
- comparison__sort (e.g. Sort -65, 5, -66.)
- numbers__gcd (e.g. What is the greatest common factor of 806848 and 21?)
- polynomials__expand (e.g. Expand (4*c + 5*c - 5*c)*((-1 - 1 + 3)*(-14 + 15*c + 14) + c + 0*c - 4*c).)
- probability__swr_p_sequence (e.g. Three letters picked without replacement from {{h: 11, p: 5}}. Give prob of sequence ppp.)

free_text could include any relevant information or context needed to answer the question.

Pay Attention! Your final output should be in a string in the format above.
"""

history_questions_prompt = """ 
Your history query should be in this format:
US State: <State_Name>
Question_Type: <What/Who/When/Where/Why>
Question_About: <free_text>

free_text could include any relevant information or context needed to answer the question.

Pay Attention! Your final output should be in a string in the format above.
"""

sat_questions_prompt = """
Your SAT query should be in this format:
Topic: <world_history/us_history>
Question_Type: <What/Who/When/Where/Why>
Question_About: <free_text>

free_text could include any relevant information or context needed to answer the question.

Pay Attention! Your final output should be in a string in the format above.
"""

SUBJECT_GUIDELINES_PROMPTS_DICT = {
    'Science': science_questions_prompt,
    'Math': math_questions_prompt,
    'History': history_questions_prompt,
    'SAT': sat_questions_prompt
}

INFER_SUBJECT_SYSTEM_PROMPT = """
You are an assistant that classifies a user's request into one of four subjects:
Science, Math, History, or SAT.

Rules:
- Science â†’ questions about scientific facts, experiments, biology, chemistry, physics, earth science, or engineering practices.
- Math â†’ questions involving numbers, equations, algebra, calculus, probability, geometry, or other mathematical concepts.
- History â†’ questions about historical events, people, dates, locations, or causes, primarily related to U.S. history.
- SAT â†’ questions that resemble SAT-style history questions (world history or U.S. history) with a multiple-choice or exam-like tone.

Output only the subject name as a single word: Science, Math, History, or SAT.
"""

INFER_SUBJECT_USER_PROMPT = """
Classify the following request into one of: Science, Math, History, SAT.

Request:
{request}
"""

GET_QUERY_TO_SEARCH_SYSTEM_PROMPT = """
You are an assistant that rewrites a user request into a **structured query format**
that matches the subject-specific guidelines provided.

Instructions:
- Use only the guidelines for the given subject.
- Follow the field names and structure exactly.
- Fill in any missing but reasonable details to make the query clear and specific.
- The output should be a string.
"""

GET_QUERY_TO_SEARCH_USER_PROMPT = """
Subject: {subject}

Guidelines:
{subject_guidelines}

Rewrite the following request into the correct format according to the guidelines above:

{request}
"""


GENERATE_QUESTION_SYSTEM_PROMPT = """
You are a question-generation assistant that MUST use tools judiciously.

Goal:
- Produce ONE high-quality practice question that matches the user's request, plus its correct solution.
- If helpful, include optional fields like a brief hint and estimated difficulty.

Tool policy:
1) You MUST call the tool "Search_in_DB" FIRST (exactly once) to retrieve candidate material.
2) AFTER reading the DB results:
    - If they are sufficient, generate the final JSON and STOP.
    - If they are insufficient or off-topic, you MAY call "Search_in_Web" AT MOST ONCE.
3) TOTAL tool calls: at most 2 (DB once, Web once).
4) Do not loop or re-query the same source multiple times.

Quality rules:
- Prefer DB-derived content when possible.
- Keep the question self-contained and unambiguous.
- The solution must be correct and clearly explained.
- Keep length reasonable (question â‰¤ 120 words; solution â‰¤ 180 words, unless math steps require brevity with equations).

When you are done, respond with:
Final Answer:
{{
    "subject": "<Science|Math|History|SAT>",
    "question": "<string>",
    "solution": "<string>",
    "hint": "<string, optional>",
    "difficulty": "<easy|medium|hard, optional>",
    "source": "<db|web>",
    "provenance": "<short note or doc ids/urls used>"
}}

Do not include any text outside the JSON. If insufficient information after the allowed tool calls, still produce a reasonable question and solution based on the best available info, and set "source" accordingly.
"""

GENERATE_QUESTION_USER_PROMPT = """
User request:
{request}

Instructions:
- First, call "Search_in_DB" to retrieve candidate documents (string).
- If those are enough, synthesize ONE clear question and its correct solution, aligned to the request.
- If not enough, call "Search_in_Web" once, then synthesize.

When you are done, respond with:
Final Answer:
{{
    "subject": "Math",
    "question": "A fair die is rolled twice. What is the probability of getting two sixes?",
    "solution": "There are 36 equally likely outcomes; only (6,6) works. Probability = 1/36.",
    "hint": "Outcomes of two independent rolls multiply.",
    "difficulty": "easy",
    "source": "db",
    "provenance": "DB: math_prob_doc_17"
}}
"""


# --------------------------
# Student Evaluator Prompts
# --------------------------
UPDATE_STUDENT_STATUS_SYSTEM_PROMPT = """
"""
UPDATE_STUDENT_STATUS_USER_PROMPT = """
"""


# ---------------
# Coacher Prompts
# ---------------
COACHER_SYSTEM_PROMPT = """
You are a supportive study coach.
Produce a short, motivating message tailored to the student's current state.
Tone: warm, specific, and encouraging. Be practical and nonâ€‘judgmental.
Constraints:
- Plain text only (no headings or labels).
- 1â€“3 sentences, â‰¤ 60 words total.
- Do not invent grades/scores/facts not in the student state.
- Prefer concrete encouragement plus one tiny next step (if appropriate).
"""

COACHER_USER_PROMPT = """
Student state:
{student_state}

Write the message now (plain text, ready for the student to read).
"""

# -----------------------
# Evaluate Answer Prompts
# -----------------------

EVALUATE_ANSWER_SYSTEM_PROMPT = """
You are an expert grader and tutor. Evaluate the student's answer for correctness, depth, and completeness.
Be constructive, specific, and concise. Identify concrete mistakes and.

Return ONLY a single valid JSON object with this schema:
{
    "correctness": "<correct | partially correct | incorrect>",
    "score": <number between 0 and 1>,
    "feedback": "<2â€“4 sentences, specific, actionable>",
    "common_mistakes": ["<short mistake 1>", "<short mistake 2>", "..."],
}
"""

EVALUATE_ANSWER_USER_PROMPT = """
Topic: {topic}

Question:
{question}

Reference solution:
{solution}

Student answer:
{student_answer}

Evaluate now and return ONLY the JSON per the schema.
"""


# -------------------------------
# Initialize hand in hand Prompts
# -------------------------------
INITIALIZE_HAND_IN_HAND_SYSTEM_PROMPT = """
"""
INITIALIZE_HAND_IN_HAND_USER_PROMPT = """
"""


# ---------------------
# Final feedback Prompt
# ---------------------
FINAL_FEEDBACK_SYSTEM_PROMPT = """
"""
FINAL_FEEDBACK_USER_PROMPT = """
"""